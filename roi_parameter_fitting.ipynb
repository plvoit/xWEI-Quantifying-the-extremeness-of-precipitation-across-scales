{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ROI parameter fitting\n",
    "Copyright (c) 2021-, Paul Voit\n",
    "Creative Commons Attribution 4.0 International License\n",
    "12 May 2022, voit@uni-potsdam.de\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The parameters are calculated according to Burn (1990). For every cell we defined the surounding pixels in a 19 x 19 area as neighbors. The user has to specify a weighing function which defines how much the influence of neighboring decreases with increasing distance. We used follwoing formula: $$mu=1-(\\frac{d}{26})$$ with *d* as the distance to be weighted.\n",
    "The parameters for the event (\"data/roi_parameters\") were calculated with the following script. The only difference is, that because of the size of the region of interest values on the sides (first and last 9 rows, first and last 9 columns) are not identical. This is because for the paper the ROI parameters were calcualted for whole Germany. Because were using a subset here, the values on the sides for the ROI are missing (moving window).  Because the process takes some time (about one hour), the actual calculation in the Example-notebook loads the parameters, which are subset from the Germany wide parameter set, and does not calculate them again. The calculation is based on the yearly maxima for each duration (\"data/yearmax_2001_2020\").\n",
    "Sometimes, when calculating the Eta-ROI numpy throws a runtime warning. This happens when there is there is the root of a negative number calculated, which should probably not happen. This returns \"nan\". Because of this and because we can't check the timeseries of every pixel, we think, that the ROI parameter calculation is less reliable than the other methods. These warnings just occur sometimes and can be ignored. Despite these inconsistencies the results achieved with this method are very similar to the results achieved with the other methods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import pandas\n",
    "import os\n",
    "import xarray as xr"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def weighting_f(d, n, tp):\n",
    "    \"\"\"\n",
    "    Function to calculate the weights based on the distance to the center of the raster for ROI-method. Center has weight one.\n",
    "    This approach is based on the paper of Burn [1990]\n",
    "    :param d: distance to be weighted, float\n",
    "    :param n: parameter 1, have to be checked so there  won't be negative weights. This depends on the choosen distance\n",
    "    :param tp: parameter 2, have to be checked so there  won't be negative weights. This depends on the choosen distance\n",
    "    :return: weight\n",
    "    \"\"\"\n",
    "    mu = 1 - (d / tp) ** n\n",
    "    return mu\n",
    "\n",
    "def PWM(x):\n",
    "    \"\"\"\n",
    "    Probability weighted moments for a series of data points. According to Burn (1990)\n",
    "    :param x: series of data points, float\n",
    "    :return: tuple of the first three moments of the distribution\n",
    "    \"\"\"\n",
    "    # np number of years, x timeseries at point\n",
    "    # sort x?? Hosking et al. Yes\n",
    "    x = np.array(sorted(x))\n",
    "\n",
    "    moments = []\n",
    "    for i in range(0, 3):\n",
    "        p = [((j - 0.35) / len(x)) ** i for j in range(1, len(x) + 1)]\n",
    "        p = np.array(p)\n",
    "        mr = (1 / len(x)) * sum(p * x)\n",
    "        moments.append(mr)\n",
    "\n",
    "    return moments\n",
    "\n",
    "def dist(p1, p2):\n",
    "    \"\"\"\n",
    "    Euclidian distance between two 2D points\n",
    "    :param p1: tuple, array index (2D)\n",
    "    :param p2: tuple, array index (2D)\n",
    "    :return: distance array\n",
    "    \"\"\"\n",
    "    dx = p2[0] - p1[0]\n",
    "    dy = p2[1] - p1[1]\n",
    "    d = (dy ** 2 + dx ** 2) ** 0.5\n",
    "    return d\n",
    "\n",
    "\n",
    "def roi(yearmax_path, roi_size):\n",
    "    '''\n",
    "    Calculates the ROI-parameters for each duration\n",
    "    :param path2yearmax: String\n",
    "        FIlepath to the ncdf files that contain the yearly maxima for every duration\n",
    "    :param roi_size: Int\n",
    "        Window size of the Region of Interest\n",
    "\n",
    "    :return: dictionary\n",
    "        Dictionary which contains for each duration another dictionary containing the four parameters for the ROI-method\n",
    "    '''\n",
    "    center = roi_size // 2\n",
    "    files = os.listdir(yearmax_path)\n",
    "    durations = [int(file[:2]) for file in files ]\n",
    "    parms_dict = dict.fromkeys(sorted(durations))\n",
    "\n",
    "    #create the distance array\n",
    "    distance_array = np.zeros((roi_size, roi_size))\n",
    "\n",
    "    for row in range(roi_size):\n",
    "        for col in range(roi_size):\n",
    "            distance_array[row, col] = dist([center, center], [row, col])\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        print(f\"{datetime.datetime.now()} Fitting ROI parameters for duration {file[:2]} h.\")\n",
    "        nc = xr.open_dataset(f\"{yearmax_path}/{file}\")\n",
    "        nc = nc['rainfall_amount'].values\n",
    "        nc = nc[:20, :, :]\n",
    "\n",
    "        # pad with nan so no values will be lost. Otherwise np.slinding_window will return a smaller array\n",
    "        nc = np.pad(nc, center, 'constant', constant_values=np.nan)\n",
    "        # remove pads for the 3rd(time) dimension\n",
    "        nc = nc[center:nc.shape[0] - center, :, :]\n",
    "\n",
    "        #create dictionary to store all the arrays\n",
    "        parm_names = [\"g\", \"xi\", \"alpha\", \"m0\"]\n",
    "        res = dict.fromkeys(parm_names)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Using this moving window approach one can save the padding when splitting up the array for multiprocessing.\n",
    "        Basically for each cell in the array there is the window stored as additionally dimensions. The window/cube for\n",
    "        each respective cell (x,y) can be accessed by: file[:, x, y, :, :]\n",
    "        \"\"\"\n",
    "        nc = np.lib.stride_tricks.sliding_window_view(nc, window_shape=(roi_size, roi_size), axis=(1, 2))\n",
    "\n",
    "        # arrays to store the results\n",
    "        g_store = np.empty(shape=(nc.shape[1], nc.shape[2]))\n",
    "        g_store[:] = np.nan\n",
    "        xi_store = np.empty(shape=(nc.shape[1], nc.shape[2]))\n",
    "        xi_store[:] = np.nan\n",
    "        alpha_store = np.empty(shape=(nc.shape[1], nc.shape[2]))\n",
    "        alpha_store[:] = np.nan\n",
    "        m0_store = np.empty(shape=(nc.shape[1], nc.shape[2]))\n",
    "        m0_store[:] = np.nan\n",
    "\n",
    "        for row in range(nc.shape[1]):\n",
    "            for col in range(nc.shape[2]):\n",
    "                weight_dist = weighting_f(distance_array, 1, 26)\n",
    "                r = nc[:, row, col, :, :]\n",
    "\n",
    "                # if np.isnan(r[:, center, center]).sum() != 0:\n",
    "                if any(np.isnan(r[:, center, center])):\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    pwm_grid = np.zeros((3, roi_size, roi_size))\n",
    "\n",
    "                    for m in range(0, pwm_grid.shape[1]):\n",
    "                        for n in range(0, pwm_grid.shape[2]):\n",
    "                            if any(np.isnan(r[:, m, n])):\n",
    "\n",
    "                                pwm_grid[0, m, n] = np.nan\n",
    "                                pwm_grid[1, m, n] = np.nan\n",
    "                                pwm_grid[2, m, n] = np.nan\n",
    "                                weight_dist[m, n] = np.nan\n",
    "\n",
    "                            else:\n",
    "                                pw_moments = PWM(r[:, m, n])\n",
    "                                pwm_grid[0, m, n] = pw_moments[0]\n",
    "                                pwm_grid[1, m, n] = pw_moments[1]\n",
    "                                pwm_grid[2, m, n] = pw_moments[2]\n",
    "\n",
    "                    t1 = pwm_grid[1] / pwm_grid[0]\n",
    "                    t2 = pwm_grid[2] / pwm_grid[0]\n",
    "\n",
    "                    T1 = (np.nansum(t1 * r.shape[0] * weight_dist)) / (np.nansum(r.shape[0] * weight_dist))\n",
    "                    T2 = (np.nansum(t2 * r.shape[0] * weight_dist)) / (np.nansum(r.shape[0] * weight_dist))\n",
    "\n",
    "                    c = (2 * T1 - 1) / (3 * T2 - 1) - math.log(2) / math.log(3)\n",
    "                    g = 7.859 * c + 2.955 * c ** 2\n",
    "                    alpha = (2 * T1 - 1) * g / (math.gamma(1 + g) * (1 - 2 ** -g))\n",
    "                    xi = 1 + alpha * (math.gamma(1 + g) - 1) / g\n",
    "\n",
    "                    # needed for calculation of return period is also the mean value of r[:,center,center]\n",
    "\n",
    "                    g_store[row, col] = g\n",
    "                    xi_store[row, col] = xi\n",
    "                    alpha_store[row, col] = alpha\n",
    "                    m0_store[row, col] = r[:, center, center].mean()\n",
    "\n",
    "        # Writing the parameters to file\n",
    "        # dummy = pd.DataFrame(g_store)\n",
    "        # dummy.to_csv(f\"roi_g_{file[:2]}.csv\", header=False, index=False)\n",
    "        #\n",
    "        # dummy = pd.DataFrame(xi_store)\n",
    "        # dummy.to_csv(f\"roi_xi_{file[:2]}.csv\", header=False, index=False)\n",
    "        #\n",
    "        # dummy = pd.DataFrame(alpha_store)\n",
    "        # dummy.to_csv(f\"roi_alpha_{file[:2]}.csv\", header=False, index=False)\n",
    "        #\n",
    "        # dummy = pd.DataFrame(m0_store)\n",
    "        # dummy.to_csv(f\"roi_m0_{file[:2]}.csv\", header=False, index=False)\n",
    "\n",
    "        res[\"g\"] = g_store\n",
    "        res[\"xi\"] = xi_store\n",
    "        res[\"alpha\"] = alpha_store\n",
    "        res[\"m0\"] = m0_store\n",
    "\n",
    "        current_duration = file[:2]\n",
    "        parms_dict[current_duration] = res\n",
    "\n",
    "    return parms_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-12 09:38:44.493265 Fitting ROI parameters for duration 01 h.\n",
      "CPU times: user 9min 30s, sys: 16.1 s, total: 9min 47s\n",
      "Wall time: 9min 29s\n"
     ]
    }
   ],
   "source": [
    "# calculate the ROI parameters\n",
    "roi_parms = roi(\"data/yearmax_2001_2020\", 19)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}